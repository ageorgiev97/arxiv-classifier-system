#!/usr/bin/env python
import os

import argparse
import logging
import sys
from pathlib import Path

# Ensure src is in the path if not installed as a package
sys.path.append(str(Path(__file__).resolve().parent.parent))

from src.arxiv_classifier.config import settings
from src.arxiv_classifier.training import ArxivTrainer
from src.arxiv_classifier.models import SciBertClassifier, BaselineClassifier, SpecterClassifier
from src.arxiv_classifier.data.loader import load_hf_dataset

import tensorflow as tf
import wandb
try:
    from wandb.integration.keras import WandbMetricsLogger
except ImportError:
    try:
        from wandb.keras import WandbMetricsLogger
    except ImportError:
        WandbMetricsLogger = None


logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def main():
    parser = argparse.ArgumentParser(description="ArXiv Article Classification - Training Entry Point")
    parser.add_argument("--model_type", type=str, choices=["scibert", "baseline", "specter"], default="scibert")
    parser.add_argument("--run_name", type=str, help="Name for the W&B run", default="scibert_v1")
    parser.add_argument("--epochs", type=int, default=settings.model.epochs)
    parser.add_argument("--batch_size", type=int, default=settings.model.batch_size)
    parser.add_argument("--limit_samples", type=int, default=10000, help="Limit dataset size for debugging")
    args = parser.parse_args()

    # 1. Load Category Mappings (Generated by EDA)
    settings.load_category_config()
    num_classes = len(settings.id2label)

    # 2. Instantiate the specific Model
    if args.model_type == "scibert":
        model = SciBertClassifier(
            num_classes=num_classes, 
            model_name=settings.model.model_name
        )
        logger.info(f"Initialized SciBERT model ({settings.model.model_name}) for {num_classes} classes.")
    elif args.model_type == "specter":
        model = SpecterClassifier(
            num_classes=num_classes
        )
        logger.info(f"Initialized Specter model (allenai/specter) for {num_classes} classes.")
    else:
        model = BaselineClassifier(num_classes=num_classes)
        logger.info("Initialized TF-IDF Baseline model.")

    # 3. Load Data using our standard loader
    train_raw = load_hf_dataset("train")
    val_raw = load_hf_dataset("val")

    # Optional: Limit samples for faster experimentation/debugging
    if args.limit_samples:
        logger.info(f"Limiting dataset to first {args.limit_samples} samples.")
        # Ensure we don't exceed actual length
        train_limit = min(args.limit_samples, len(train_raw))
        val_limit = min(args.limit_samples, len(val_raw))
        
        train_raw = train_raw.select(range(train_limit))
        val_raw = val_raw.select(range(val_limit))

    # Check for GPU/MPS
    gpus = tf.config.list_physical_devices('GPU')
    if gpus:
        logger.info(f"Found {len(gpus)} GPU(s): {gpus}")
        logger.info("MPS (Metal Performance Shaders) acceleration is active.")
    else:
        logger.warning("No GPU found. Training will be slow on CPU.")

    # 4. Preprocess Data

    vocab_size = 20000
    if args.model_type == "baseline":
        # Import text preprocessing from baseline module
        from src.arxiv_classifier.models.baseline import preprocess_texts_batch
        
        # Initialize TextVectorization here for baseline
        logger.info("Preparing TextVectorization for baseline...")
        vectorizer = tf.keras.layers.TextVectorization(
            max_tokens=vocab_size,
            output_mode='tf_idf'
        )

        def preprocess_for_adaptation(examples):
            # Combine title and abstract
            texts = [f"{t} [SEP] {a}" for t, a in zip(examples['title'], examples['abstract'])]
            # Apply NLP preprocessing (lemmatization, stop-word removal)
            return preprocess_texts_batch(texts, use_lemmatization=True, use_stemming=False, use_stopwords=True)

        logger.info("Applying NLP preprocessing (lemmatization, stop-word removal) to training data...")
        # Map to verify text format
        to_adapt = train_raw.map(
            lambda x: {"text": preprocess_for_adaptation(x)},
            batched=True,
            remove_columns=train_raw.column_names
        )
        
        # Convert to list for adaptation (simpler than tf.data)
        adaptation_texts = to_adapt["text"]

        # Adapt on CPU
        logger.info("Adapting vectorizer...")
        with tf.device("/CPU:0"):
            vectorizer.adapt(adaptation_texts)
            
        # Save vectorizer as a complete model to preserve TF-IDF state
        # Use SavedModel format (directory) instead of .keras for robust asset handling
        vec_path = settings.paths.ARTIFACTS_DIR / f"{args.run_name}_vectorizer_tf"
        vec_model = tf.keras.Sequential([vectorizer])
        # Build the model by passing dummy data
        vec_model.predict(tf.constant(["dummy text"]), verbose=0)
        tf.saved_model.save(vec_model, str(vec_path))
        logger.info(f"Saved vectorizer to {vec_path}")
        
        # Now define mapping that includes vectorization
        def preprocess_baseline_vectorized(examples):
            texts = preprocess_for_adaptation(examples)
            # Vectorize on CPU to avoid MPS issues if any
            with tf.device("/CPU:0"):
                vectors = vectorizer(texts)
            
            # Multi-hot encode categories
            labels = []
            for cats in examples['categories']:
                multi_hot = [0.0] * num_classes
                for cat in cats:
                    if cat in settings.label2id:
                        multi_hot[settings.label2id[cat]] = 1.0
                labels.append(multi_hot)
            
            return {"text": vectors.numpy(), "labels": labels}

        logger.info("Vectorizing and preprocessing datasets...")
        train_tokenized = train_raw.map(preprocess_baseline_vectorized, batched=True, remove_columns=train_raw.column_names)
        val_tokenized = val_raw.map(preprocess_baseline_vectorized, batched=True, remove_columns=val_raw.column_names)
        
        cols = ['text']
        
    else:
        # Transformer preprocessing
        from transformers import AutoTokenizer
        tokenizer = AutoTokenizer.from_pretrained(settings.model.model_name)

        def preprocess_transformer(examples):
            # Concatenate title and abstract
            texts = [f"{t} [SEP] {a}" for t, a in zip(examples['title'], examples['abstract'])]
            
            # Tokenize
            result = tokenizer(
                texts, 
                padding='max_length', 
                truncation=True, 
                max_length=settings.model.max_length
            )

            # Multi-hot encode categories
            labels = []
            for cats in examples['categories']:
                multi_hot = [0.0] * num_classes
                for cat in cats:
                    if cat in settings.label2id:
                        multi_hot[settings.label2id[cat]] = 1.0
                labels.append(multi_hot)
            
            result["labels"] = labels
            return result

        logger.info("Preprocessing datasets for transformer (tokenization)...")
        train_tokenized = train_raw.map(preprocess_transformer, batched=True, remove_columns=train_raw.column_names)
        val_tokenized = val_raw.map(preprocess_transformer, batched=True, remove_columns=val_raw.column_names)
        
        # For transformer, we pass tokenized inputs
        cols = ['input_ids', 'attention_mask']

    # Convert to TF datasets
    train_ds = train_tokenized.to_tf_dataset(
        columns=cols,
        label_cols=['labels'],
        shuffle=True,
        batch_size=args.batch_size
    )
    val_ds = val_tokenized.to_tf_dataset(
        columns=cols,
        label_cols=['labels'],
        shuffle=False,
        batch_size=args.batch_size
    )



    # Optimizations for training speed
    AUTOTUNE = tf.data.AUTOTUNE
    train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)
    val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)

    # 5. Initialize Trainer with structured config
    callbacks = []
    if args.run_name:
        wandb.init(project="arxiv-classifier", name=args.run_name, config=settings.model.model_dump())
        if WandbMetricsLogger:
            callbacks.append(WandbMetricsLogger())
            
    config_dict = settings.model.model_dump()
    config_dict.update({"epochs": args.epochs, "batch_size": args.batch_size})
    
    trainer = ArxivTrainer(model=model, config=config_dict)

    # 6. Execute Training
    logger.info(f"Starting {args.model_type} training run: {args.run_name}")
    history = trainer.compile_and_fit(
        train_ds=train_ds, 
        val_ds=val_ds,
        callbacks=callbacks
    )

    # 7. Log final metrics to WandB
    if wandb.run is not None:
        final_metrics = {f"final_{k}": v[-1] for k, v in history.history.items()}
        wandb.log(final_metrics)
        wandb.summary.update(final_metrics)
        logger.info(f"Logged final metrics to WandB: {list(final_metrics.keys())}")

    # 8. Save Keras model
    keras_output_path = settings.paths.ARTIFACTS_DIR / f"{args.run_name}.keras"
    model.save(keras_output_path)
    logger.info(f"Saved Keras model to: {keras_output_path}")
    
    # Log Keras model as WandB artifact
    if wandb.run is not None:
        keras_artifact = wandb.Artifact(name=f"{args.run_name}_keras", type="model")
        keras_artifact.add_file(str(keras_output_path))
        if args.model_type == "baseline":
            # Also log vectorizer model for baseline
            vec_path = settings.paths.ARTIFACTS_DIR / f"{args.run_name}_vectorizer_tf"
            keras_artifact.add_dir(str(vec_path), name="vectorizer_tf")
        wandb.log_artifact(keras_artifact)
        logger.info("Keras model logged as WandB artifact")

    # Finish WandB run
    if wandb.run is not None:
        wandb.finish()

if __name__ == "__main__":
    main()